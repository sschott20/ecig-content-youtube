# -*- coding: utf-8 -*-
"""promotion_classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-UXP3RsQ1ioAE-4RICqSUMQCt4wv8N4b
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# %cd /content
# !rm -rf vape-classification/
# !git clone https://github.com/sschott20/youtube-vape-classification
# %cd youtube-vape-classification
# %mkdir 'models'

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# 
# 
# import os
# import tensorflow as tf
# import pickle
# import keras
# from numpy import array
# from keras import models
# from keras.preprocessing.text import one_hot
# from keras.preprocessing.sequence import pad_sequences
# from keras.models import Sequential
# from keras.layers.core import Activation, Dropout, Dense
# from keras.layers import Flatten, LSTM, Bidirectional
# from keras.layers import GlobalMaxPooling1D
# from keras.models import Model
# from keras.layers.embeddings import Embedding
# from sklearn.model_selection import train_test_split
# from keras.preprocessing.text import Tokenizer
# from keras.layers import Input
# from keras.layers.merge import Concatenate
# !pip install langdetect
# from langdetect import detect, detect_langs
# from keras.utils import np_utils
# 
# from pydrive.auth import GoogleAuth
# from pydrive.drive import GoogleDrive
# from google.colab import auth
# from oauth2client.client import GoogleCredentials
# 
# from zipfile import ZipFile
# import nltk
# from sklearn.preprocessing import LabelEncoder, OneHotEncoder
# import pandas as pd
# import numpy as np
# import re
# import random
# import string
# import requests
# from tqdm import tqdm
# from bs4 import BeautifulSoup
# from nltk.corpus import stopwords
# nltk.download('stopwords')
#

with open('/content/youtube-vape-classification/data.p', 'rb') as f: 
    data = pickle.loads(f.read())

for d in data:
    if len(d[1].split(' ')) > 300:
        print(d)

auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

download = drive.CreateFile({'id': '1Pcow88UospIZfFX-qhEPt0wPs6EmwW0X'})
download.GetContentFile('/content/youtube-vape-classification/glove_small.zip')

with ZipFile('/content/youtube-vape-classification/glove_small.zip', 'r') as zipObj:
   zipObj.extractall()

embeddings_dictionary = dict()

with open('/content/youtube-vape-classification/glove.twitter.27B.100d.txt') as f:
    for line in f:
        values = line.split()
        word = values[0]
        vector_dimensions = np.asarray(values[1:], dtype='float32')
        embeddings_dictionary[word] = vector_dimensions

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
cachedStopWords = stopwords.words("english")
maxlen = 300

def preprocess_text(input_text):
    
    output_text = input_text.lower()
    output_text = re.sub(r'(https?:\/\/)(\s)*(www\.)?(\s)*((\w|\s)+\.)*([\w\-\s]+\/)*([\w\-]+)((\?)?[\w\s]*=\s*[\w\%&]*)*', ' link ', output_text)
    output_text = re.sub(r'[^a-zA-Z0-9 ]', ' ', output_text)
    output_text = re.sub(r'[0-9]+', ' number ', output_text)
    output_text = re.sub(r'\s+', ' ', output_text)

    output_text = re.sub(r'\s+[a-zA-Z]\s+', ' ', output_text)

    output_text = ' '.join([word for word in output_text.split() if word not in cachedStopWords])

    # output_text = re.sub(r'\s+[f-zF-Z]\s+', ' ', output_text)
    return output_text

def process_text(input_text):
    output_text = tokenizer.texts_to_sequences(input_text)
    output_text = pad_sequences(output_text, padding='post', truncating='post', maxlen=maxlen)
    return output_text

readable_data = pd.read_excel('/content/youtube-vape-classification/condensed_data.xlsx')
# curled_descriptions = []
data = []

for i, row in readable_data.iterrows():
    print(i)
    meta_descriptions = []
    meta_titles = []
    label = row['model_4'].split()
   
    
    raw_links = re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', str(row['Description']))
    raw_links = [link for link in raw_links if 'facebook' not in link and 'paypal' not in link and 'youtube' not in link and 'twitter' not in link and 'amzn' not in link and 'instagram' not in link and 'youtu.be' not in link and 'amazon' not in link and 'patreon' not in link]
    
    if len(raw_links) > 5:
        raw_links = raw_links[:6]
    for link in raw_links:
        
            if link[-1] == '.' or link[-1] == ')':
                link = link[:-1]
            try:
                response = requests.get(link, allow_redirects=True, timeout=9.05)
                soup = BeautifulSoup(response.text)
                
               

                meta_description = soup.find("meta", property="og:description")

                if meta_description is not None:
                    meta_descriptions.append(meta_description["content"])
                    # print(meta_description["codntent"], label, link)

                else:
                    meta_description = soup.find('meta', attrs={'name':'description'})
                    if meta_description is not None:
                        meta_descriptions.append(meta_description["content"])

    
                # metas = soup.find_all('meta')
                # meta_descriptions.append([meta.attrs['content'] for meta in metas if 'name' in meta.attrs and meta.attrs['name'] == 'description' or meta.attrs == ''][0])
                # print([meta.attrs['content'] for meta in metas if 'name' in meta.attrs and meta.attrs['name'] == 'description' or meta.attrs == ''])
            except Exception as e:
                
                print(e, link)
                continue

    if meta_descriptions != []:
        x = preprocess_text(' '.join(meta_descriptions))  + ' ' + preprocess_text(str(row['Description']))
        # print(label, ' '.join(meta_descriptions))
        # +  preprocess_text(str(row['Video_Title'])) +
    else:
        x = preprocess_text(str(row['Description']))
        # print(label)

    if label != ['n'] and label != ['n', 'n'] and label != ['1', 'n', 'n']:
  
        if detect(x) == 'en':
            data.append([' '.join(label), x])
            # print(label, x)
            pass

import pickle 
pickle.dump(data, open( "data.p", "wb" ))

for i in data: print(i)

t_data = data.copy()

for d in t_data:
    if '0' in d[0]:
        data.append(d)
    if '2' in d[0]:
        data.append(d)

num_words = 900

data_x = [i[1] for i in data]
data_y = [i[0] for i in data]

tokenizer = Tokenizer(num_words=num_words)
tokenizer.fit_on_texts(data_x)
for i, n in enumerate(tokenizer.word_index):
    if i > 1500:
        break
    print(i, n, tokenizer.word_counts[n])

embedding_matrix = np.zeros((num_words, 100))
for word, index in tokenizer.word_index.items():
    if index > num_words - 1:
        break
    embedding_vector = embeddings_dictionary.get(word)    
    if  embedding_vector is not None:
        embedding_matrix[index] = embedding_vector

template = [0, 0, 0, 0]
dummy_y = []
for i in data_y:
    append = template.copy()

    for t in i.split(" "):
        # print(i)
        append[int(t)] = 1

    dummy_y.append(append)
dummy_y = np.array(np.array(dummy_y))
# print(dummy_y[10:110])
# dummy_y = np.ndarray(dummy_y)

model = Sequential()
model.add(Embedding(num_words, 100, weights=[embedding_matrix], trainable=True))
model.add(Bidirectional(LSTM(128)))
# model.add(LSTM(64))
model.add(Dense(4, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc', keras.metrics.Precision(), keras.metrics.Recall()])

x_train_text, x_test_text, y_train, y_test = train_test_split(data_x, dummy_y, test_size = 0.2, random_state=2)


batch_size = 16
epochs = 8

x_train = process_text(x_train_text)
x_test = process_text(x_test_text)

history = model.fit(x=x_train, y=y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))

import matplotlib.pyplot as plt

def plot_history(history):
    acc = history.history['acc']
    val_acc = history.history['val_acc']
    loss = history.history['loss']
    val_loss = history.history['val_loss']
    x = range(1, len(acc) + 1)

    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(x, acc, 'b', label='Training acc')
    plt.plot(x, val_acc, 'r', label='Validation acc')
    plt.title('Training and validation accuracy')
    plt.legend()
    plt.subplot(1, 2, 2)
    plt.plot(x, loss, 'b', label='Training loss')
    plt.plot(x, val_loss, 'r', label='Validation loss')
    plt.title('Training and validation loss')
    plt.legend()
plot_history(history=history)

from statistics import mean, harmonic_mean

model_stats = pd.DataFrame

template = {
    "false_negative": 0,
    "true_negative": 0,
    "false_positive": 0,
    "true_positive": 0,
}
# print(y_test[0])
count_list = [template.copy() for i in y_test[0]]

results = model.predict(x=x_test)
threshhold = 0.45


for i, result in enumerate(results):
    correct = True
    error = []

    rounded_result = [round(i, 2) for i in result]

    
    
    


    for label in range(len(count_list)):

        if result[label] > threshhold  and y_test[i][label] == 1:
            count_list[label]['true_positive'] += 1
            
        elif result[label] > threshhold  and y_test[i][label] == 0:
            count_list[label]['false_positive'] += 1
            
            

        elif result[label] < threshhold  and y_test[i][label] == 0:
            count_list[label]['true_negative'] += 1
            


        elif result[label] < threshhold  and y_test[i][label] == 1:
            count_list[label]['false_negative'] += 1
            if label == 0:
                print(y_test[i], rounded_result, x_test_text[i])
    # if correct == False:
    #     if y_test[i][5] == 1:
            # print(y_test[i], rounded_result, x_test_text[i])
all_precision = []
all_recall = []
all_f1 = []
combined = template.copy()

for i in range(len(count_list)):
    try:
        precision = count_list[i]['true_positive'] / (count_list[i]['true_positive'] + count_list[i]['false_positive'])
        recall = count_list[i]['true_positive'] / (count_list[i]['true_positive'] + count_list[i]['false_negative'])
        f1 = 2 * (precision * recall) / (precision + recall)
        
        combined['true_positive'] += count_list[label]['true_positive']
        combined['true_negative'] += count_list[label]['true_negative']
        combined['false_positive'] += count_list[label]['false_positive']
        combined['false_negative'] += count_list[label]['false_negative']

        all_precision.append(round(precision,2))
        all_recall.append(round(recall,2))
        all_f1.append(round(f1,2))
    except:
        all_precision.append(0)
        all_recall.append(0)
        all_f1.append(0)
precision = combined['true_positive'] / (combined['true_positive'] + combined['false_positive'])
recall = combined['true_positive'] / (combined['true_positive'] + combined['false_negative'])
f1 = 2 * (precision * recall) / (precision + recall)    
# print(sum(all_precision) / len(all_precision))
# print(sum(all_recall) / len(all_recall))
# print(sum(all_f1) / len(all_f1))
print(precision, recall, f1)

for i in range(len(count_list)):
    print(i)
    print(count_list[i])
    print(all_precision[i])
    print(all_recall[i])
    print(all_f1[i])
    print('\n')

df = pd.DataFrame(count_list)
df['precision'] = all_precision
df['recall'] = all_recall
df['f1'] = all_f1
df.to_csv('model_stats.csv')
print(df)

with open('/content/youtube-vape-classification/predict_text.p', 'rb') as f: 
    predict_text = pickle.loads(f.read())
print(predict_text[-1])

vape_data = pd.read_csv('/content/vape-metadata.csv')
keep_columns = vape_data.columns[:13]
vape_data = vape_data[keep_columns]
headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.76 Safari/537.36'}
import time
# STOPPED AT 1795

predict_text = []

# with open('/content/youtube-vape-classification/predict_text.p', 'rb') as f: 
#     predict_text = pickle.loads(f.read())


all_links = []


for i, row in vape_data.iterrows():
    # if i < 1796:
    #     continue
    start_time = time.time()
    
    meta_descriptions = []
    
    raw_links = re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', str(row['Description']))
    raw_links = [link for link in raw_links if 'facebook' not in link and 'paypal' not in link and 'youtube' not in link and 'twitter' not in link and 'amzn' not in link and 'instagram' not in link and 'youtu.be' not in link and 'amazon' not in link and 'patreon' not in link]
    
    all_links.append(raw_links)

    if len(raw_links) > 5:
        raw_links = raw_links[:6]

    for link in raw_links:

        print(link)
        try:
            r = requests.get(link, stream=True, timeout=3.05, headers=headers)

            # print('length: ', r.headers.get('Content-Length'))

            if r.headers.get('Content-Length') is not None and int(r.headers.get('Content-Length')) > 99999:
                print(int(r.headers.get('Content-Length')))
                raise ValueError('response too large')
            
            response = requests.get(link, timeout=3.05, headers=headers)

            soup = BeautifulSoup(response.text)
            meta_description = soup.find("meta", property="og:description")


            if meta_description is not None:
                meta_descriptions.append(meta_description["content"])
            else:
                meta_description = soup.find('meta', attrs={'name':'description'})
                if meta_description is not None:
                    meta_descriptions.append(meta_description["content"])
        except Exception as e:
            print(e)

    if meta_descriptions != []:

        predict_text.append(preprocess_text(' '.join(meta_descriptions))  + ' ' + preprocess_text(str(row['Description'])))

    else:
        predict_text.append(preprocess_text(str(row['Description'])))

    print(i, time.time() - start_time)



df = pd.DataFrame(all_links)
df.to_excel('links.xlsx')

from google.colab import files
import pickle

pickle.dump(predict_text, open("predict_text_full.p", "wb" ))
files.download('predict_text_full.p')

with open('/content/predict_text_full.p', 'rb') as f: 
    predict_text = pickle.loads(f.read())

df = pd.DataFrame(predict_text)
df.to_excel('raw_text.xlsx')

predict_sequence = process_text(predict_text)
predictions = model.predict(predict_sequence, verbose=1)

for i, row in vape_data.iterrows():
    
    label = []
    for n in range(len(count_list)):
        if predictions[i][n] > threshhold:
            label.append(str(n))
    if label == []:
        label = ['3']
    
    # langs = detect_langs(predict_text[i])
    # if detect(predict_text[i]) != 'en':
    #     if str(detect_langs(predict_text[i])[0])[5:8] == '999':
    #         label = ['n']
            # print(str(detect_langs(predict_text[i])[0]), predict_text[i])
        

    for position in range(len(label)):
        vape_data.at[i, 'model_4_' + str(position + 1)] = label[position]
vape_data.to_excel('video_predictions_model_4.xlsx')