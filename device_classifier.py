# -*- coding: utf-8 -*-
"""device_classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1e8kODq5UAgZ-lgCMXPPTidg_Y8Tuf1Vx
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# %cd /content
# !rm -rf vape-classification/
# !git clone https://github.com/sschott20/youtube-vape-classification
# %cd youtube-vape-classification
# %mkdir 'models'

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# 
# 
# import os
# import tensorflow as tf
# 
# import keras
# from numpy import array
# from keras import models
# from keras.preprocessing.text import one_hot
# from keras.preprocessing.sequence import pad_sequences
# from keras.models import Sequential
# from keras.layers.core import Activation, Dropout, Dense
# from keras.layers import Flatten, LSTM, Bidirectional
# from keras.layers import GlobalMaxPooling1D
# from keras.models import Model
# from keras.layers.embeddings import Embedding
# from sklearn.model_selection import train_test_split
# from keras.preprocessing.text import Tokenizer
# from keras.layers import Input
# from keras.layers.merge import Concatenate
# 
# import matplotlib.pyplot as plt
# !pip install langdetect
# from langdetect import detect, detect_langs
# 
# from keras.utils import np_utils
# 
# from pydrive.auth import GoogleAuth
# from pydrive.drive import GoogleDrive
# from google.colab import auth
# from oauth2client.client import GoogleCredentials
# 
# from zipfile import ZipFile
# import nltk
# from sklearn.preprocessing import LabelEncoder, OneHotEncoder
# import pickle
# import pandas as pd
# import numpy as np
# import re
# import random
# import string

auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

download = drive.CreateFile({'id': '1Pcow88UospIZfFX-qhEPt0wPs6EmwW0X'})
download.GetContentFile('/content/youtube-vape-classification/glove_small.zip')

with ZipFile('/content/youtube-vape-classification/glove_small.zip', 'r') as zipObj:
   zipObj.extractall()

embeddings_dictionary = dict()

with open('/content/youtube-vape-classification/glove.twitter.27B.100d.txt') as f:
    for line in f:
        values = line.split()
        word = values[0]
        vector_dimensions = np.asarray(values[1:], dtype='float32')
        embeddings_dictionary[word] = vector_dimensions

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)

maxlen = 200

def preprocess_text(input_text):
    output_text = input_text.lower()
    output_text = re.sub(r'(https?:\/\/)(\s)*(www\.)?(\s)*((\w|\s)+\.)*([\w\-\s]+\/)*([\w\-]+)((\?)?[\w\s]*=\s*[\w\%&]*)*', ' ', output_text)
    output_text = re.sub(r'[^a-zA-Z0-9 ]', '', output_text)
    output_text = re.sub(r'[0-9]+', ' number ', output_text)
    output_text = re.sub(r'\s+', ' ', output_text)
    # output_text = re.sub(r'\s+[a-dA-D]\s+', ' ', output_text)
    # output_text = re.sub(r'\s+[f-zF-Z]\s+', ' ', output_text)
    return output_text

def process_text(input_text):
    output_text = tokenizer.texts_to_sequences(input_text)
    output_text = pad_sequences(output_text, padding='post', truncating='post', maxlen=maxlen)
    return output_text

readable_data = pd.read_excel('/content/youtube-vape-classification/condensed_data.xlsx')


data = []

for i, row in readable_data.iterrows():

    label = row['condensed_model_2'].split()
    x = preprocess_text(preprocess_text(str(row['Video_Title'])) + ' ' + preprocess_text(str(row['Description'])))
    # print(label)
    if row['model_1'] != '10':
       

        if label != ['n'] and label != ['n']:
            if len(label) < 2:
                data.append([label[0], x])
            else:
                data.append(['6', x])
        # print(label, x)

num_words = 900

data_x = [i[1] for i in data]
data_y = [i[0] for i in data]

tokenizer = Tokenizer(num_words=num_words)
tokenizer.fit_on_texts(data_x)

print(data_y[:101])

embedding_matrix = np.zeros((num_words, 100))
for word, index in tokenizer.word_index.items():
    if index > num_words - 1:
        break
    embedding_vector = embeddings_dictionary.get(word)    
    if  embedding_vector is not None:
        embedding_matrix[index] = embedding_vector

encoder = LabelEncoder()
encoder.fit(data_y)
encoded_y = encoder.transform(data_y)
dummy_y = np_utils.to_categorical(encoded_y)

model = Sequential()
model.add(Embedding(num_words, 100, weights=[embedding_matrix], trainable=True))
model.add(Bidirectional(LSTM(128)))
model.add(Dense(7, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc', keras.metrics.Precision(), keras.metrics.Recall()])

x_train_text, x_test_text, y_train, y_test = train_test_split(data_x, dummy_y, test_size = 0.2, random_state=1)
batch_size = 16
epochs = 9

x_train = process_text(x_train_text)
x_test = process_text(x_test_text)

history = model.fit(x=x_train, y=y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))

def plot_history(history):
    acc = history.history['acc']
    val_acc = history.history['val_acc']
    loss = history.history['loss']
    val_loss = history.history['val_loss']
    x = range(1, len(acc) + 1)

    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(x, acc, 'b', label='Training acc')
    plt.plot(x, val_acc, 'r', label='Validation acc')
    plt.title('Training and validation accuracy')
    plt.legend()
    plt.subplot(1, 2, 2)
    plt.plot(x, loss, 'b', label='Training loss')
    plt.plot(x, val_loss, 'r', label='Validation loss')
    plt.title('Training and validation loss')
    plt.legend()

plot_history(history)

from statistics import mean, harmonic_mean

model_stats = pd.DataFrame

template = {
    "false_negative": 0,
    "true_negative": 0,
    "false_positive": 0,
    "true_positive": 0,
}

count_list = [template.copy() for i in y_test[0]]

results = model.predict(x=x_test)


for i, result in enumerate(results):
   
    for label in range(len(count_list)):
        # print(label)
        if np.argmax(result) == label and np.argmax(y_test[i]) == label:
            count_list[label]['true_positive'] += 1
            # print('TP', label)
        elif np.argmax(result) != label and np.argmax(y_test[i]) == label:
            count_list[label]['false_negative'] += 1
            # print('FN', label)
        elif np.argmax(result) != label and np.argmax(y_test[i]) != label:
            count_list[label]['true_negative'] += 1
            # print('TN', label)
        elif np.argmax(result) == label and np.argmax(y_test[i]) != label:
            count_list[label]['false_positive'] += 1
            # print('FP', label)

all_precision = []
all_recall = []
all_f1 = []


for i in range(len(count_list)):
    try:
        precision = count_list[i]['true_positive'] / (count_list[i]['true_positive'] + count_list[i]['false_positive'])
        recall = count_list[i]['true_positive'] / (count_list[i]['true_positive'] + count_list[i]['false_negative'])
        f1 = 2 * (precision * recall) / (precision + recall)
        
        all_precision.append(round(precision, 2))
        all_recall.append(round(recall, 2))
        all_f1.append(round(f1,2))
    except:
        all_precision.append(0)
        all_recall.append(0)
        all_f1.append(0)

for i in range(len(count_list)):
    print(i)
    print(count_list[i])
    print(all_precision[i])
    print(all_recall[i])
    print(all_f1[i])
    print('\n')

df = pd.DataFrame(count_list)
df['precision'] = all_precision
df['recall'] = all_recall
df.to_csv('model_stats.csv')
print(df)

vape_data = pd.read_csv('/content/youtube-vape-classification/vape-metadata.csv')
keep_columns = vape_data.columns[:13]
vape_data = vape_data[keep_columns]
# print(vape_data.iloc[0])

predict_text = []
for i, row in vape_data.iterrows():

    predict_text.append(preprocess_text(preprocess_text(str(row['Video_Title'])) + ' ' + preprocess_text(str(row['Description']))))

predict_sequence = process_text(predict_text)
predictions = model.predict(predict_sequence, verbose=1)
# rounded_predictions = [round(i, 2) for i in predictions]

from langdetect import DetectorFactory
DetectorFactory.seed = 0

for i, row in vape_data.iterrows():
    label = np.argmax(predictions[i])
    # langs = detect_langs(predict_text[i])
    # if detect(predict_text[i]) != 'en':
    #     if str(detect_langs(predict_text[i])[0])[5:8] == '999':
    #         label = 'n'

    vape_data.at[i, 'model_2_1'] = str(label)

    
vape_data.to_excel('video_predictions_model_2.xlsx')