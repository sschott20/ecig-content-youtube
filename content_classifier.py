# -*- coding: utf-8 -*-
"""content_classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15psfk7-BveFLkXliZmMMXdmoeYxy3rT_
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# %cd /content
# !rm -rf vape-classification/
# !git clone https://github.com/sschott20/youtube-vape-classification
# !mkdir transcripts
# %cd youtube-vape-classification
# %mkdir 'models'
# !mkdir transcripts

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# 
# 
# import os
# import tensorflow as tf
# import matplotlib.pyplot as plt
# 
# import keras
# from numpy import array
# from keras import models
# from keras.preprocessing.text import one_hot
# from keras.preprocessing.sequence import pad_sequences
# from keras.models import Sequential
# from keras.layers.core import Activation, Dropout, Dense
# from keras.layers import Flatten, LSTM, Bidirectional, SimpleRNN, ConvLSTM2D,GlobalAveragePooling1D 
# from keras.models import Model
# from keras.layers.embeddings import Embedding
# from sklearn.model_selection import train_test_split
# from keras.preprocessing.text import Tokenizer
# from keras.layers import Input
# from keras.layers.merge import Concatenate
# !pip install langdetect
# from langdetect import detect, detect_langs
# from keras.utils import np_utils
# 
# from pydrive.auth import GoogleAuth
# from pydrive.drive import GoogleDrive
# from google.colab import auth
# from oauth2client.client import GoogleCredentials
# 
# from zipfile import ZipFile
# import nltk
# from sklearn.preprocessing import LabelEncoder, OneHotEncoder
# import pickle
# import pandas as pd
# import numpy as np
# import re
# import random
# import string
# 
# from langdetect import DetectorFactory
# DetectorFactory.seed = 0
#

auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

download = drive.CreateFile({'id': '1Pcow88UospIZfFX-qhEPt0wPs6EmwW0X'})
download.GetContentFile('/content/youtube-vape-classification/glove_small.zip')

with ZipFile('/content/youtube-vape-classification/glove_small.zip', 'r') as zipObj:
   zipObj.extractall()

# with ZipFile('/content/youtube-vape-classification/video_classification_data/transcripts.zip') as zipObj:
#     zipObj.extractall('/content/youtube-vape-classification/transcripts')

embeddings_dictionary = dict()

with open('/content/youtube-vape-classification/glove.twitter.27B.100d.txt') as f:
    for line in f:
        values = line.split()
        word = values[0]
        vector_dimensions = np.asarray(values[1:], dtype='float32')
        embeddings_dictionary[word] = vector_dimensions

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
from nltk.corpus import stopwords
nltk.download('stopwords')
maxlen = 200

def preprocess_text(input_text):
    output_text = input_text.lower()
    output_text = re.sub(r'(https?:\/\/)(\s)*(www\.)?(\s)*((\w|\s)+\.)*([\w\-\s]+\/)*([\w\-]+)((\?)?[\w\s]*=\s*[\w\%&]*)*', ' ', output_text)
    output_text = re.sub(r'[^a-zA-Z0-9]', ' ', output_text)
    output_text = re.sub(r'[0-9]+', ' number ', output_text)
    output_text = re.sub(r'\s+', ' ', output_text)

    pattern = re.compile(r'\b(' + r'|'.join(stopwords.words('english')) + r')\b\s*')
    output_text = pattern.sub('', output_text)
    # output_text = re.sub(r'\s+[a-dA-D]\s+', ' ', output_text)
    # output_text = re.sub(r'\s+[f-zF-Z]\s+', ' ', output_text)
    return output_text

def process_text(input_text):
    output_text = tokenizer.texts_to_sequences(input_text)
    output_text = pad_sequences(output_text, padding='post', truncating='post', maxlen=maxlen)
    return output_text

readable_data = pd.read_excel('/content/youtube-vape-classification/condensed_extra_data (new).xlsx')


data = []

for i, row in readable_data.iterrows():

    label = row['condensed_model_1'].split()
    # try:
    #     with open('/content/youtube-vape-classification/transcripts/' +  row['Video_ID'] + '.txt', 'r') as f:
    #         x = preprocess_text(f.read())
    # except:
    #     continue
    x = preprocess_text(preprocess_text(str(row['Video_Title'])) + ' ' + preprocess_text(str(row['Description'])))

    # x = preprocess_text(str(row['Video_Title']))
    if label != ['n'] and label != ['n n']:

        data.append([' '.join(label), x])
            # print(len(x.split(' ')))
        # print(label, ' '.join(label) )
        # if detect(x) != 'en':
        #     print(detect(x), x, label)

num_words = 800

data_x = [i[1] for i in data]
data_y = [i[0] for i in data]

tokenizer = Tokenizer(num_words=num_words)
tokenizer.fit_on_texts(data_x)
# for i, n in enumerate(tokenizer.word_index):
#     if i > 800:
#         break
#     print(i, n, tokenizer.word_counts[n])

embedding_matrix = np.zeros((num_words, 100))
for word, index in tokenizer.word_index.items():
    if index > num_words - 1:
        break
    embedding_vector = embeddings_dictionary.get(word)    
    if  embedding_vector is not None:
        embedding_matrix[index] = embedding_vector

template = [0, 0, 0, 0, 0, 0, 0]
dummy_y = []
for i in data_y:
    append = template.copy()

    for t in i.split(" "):
        # print(t)
        append[int(t)] = 1

    dummy_y.append(append)
dummy_y = np.array(np.array(dummy_y))
# print(dummy_y[10:110])
# dummy_y = np.ndarray(dummy_y)

model = Sequential()
model.add(Embedding(num_words, 100, weights=[embedding_matrix], trainable=True))
model.add(Bidirectional(LSTM(128)))
# model.add(SimpleRNN(128))
# model.add(GlobalAveragePooling1D())
# model.add(Dense(100, activation='relu'))
# model.add(Dense(150, activation='relu'))

# model.add(LSTM(64))
model.add(Dense(7, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc', keras.metrics.Precision(), keras.metrics.Recall()])

x_train_text, x_test_text, y_train, y_test = train_test_split(data_x, dummy_y, test_size = 0.2, random_state=2)


batch_size = 16
epochs = 10

x_train = process_text(x_train_text)
x_test = process_text(x_test_text)

history = model.fit(x=x_train, y=y_train, batch_size=batch_size, epochs=epochs, verbose=1)

from statistics import mean, harmonic_mean

model_stats = pd.DataFrame

template = {
    "false_negative": 0,
    "true_negative": 0,
    "false_positive": 0,
    "true_positive": 0,
}
# print(y_test[0])
count_list = [template.copy() for i in y_test[0]]

results = model.predict(x=x_test)
threshold = 0.4


for i, result in enumerate(results):
    correct = True
    error = []

    rounded_result = [round(i, 2) for i in result]

    # print(y_test[i], rounded_result, x_test_text[i])
    
    
    


    for label in range(len(count_list)):

        if result[label] > threshold  and y_test[i][label] == 1:
            count_list[label]['true_positive'] += 1
            
        elif result[label] > threshold  and y_test[i][label] == 0:
            count_list[label]['false_positive'] += 1
            
        elif result[label] < threshold  and y_test[i][label] == 0:
            count_list[label]['true_negative'] += 1

        elif result[label] < threshold  and y_test[i][label] == 1:
            count_list[label]['false_negative'] += 1
    
    # if correct == False:
    #     if y_test[i][5] == 1:
            # print(y_test[i], rounded_result, x_test_text[i])

all_precision = []
all_recall = []
all_f1 = []


for i in range(len(count_list)):
    try:
        precision = count_list[i]['true_positive'] / (count_list[i]['true_positive'] + count_list[i]['false_positive'])
        recall = count_list[i]['true_positive'] / (count_list[i]['true_positive'] + count_list[i]['false_negative'])
        f1 = 2 * (precision * recall) / (precision + recall)
        
        all_precision.append(round(precision,2))
        all_recall.append(round(recall,2))
        all_f1.append(round(f1,2))
    except:
        all_precision.append(0)
        all_recall.append(0)
        all_f1.append(0)
print(sum(all_precision) / len(all_precision))
print(sum(all_recall) / len(all_recall))
print(sum(all_f1) / len(all_f1))
for i in range(len(count_list)):
    
    print(i)
    print(count_list[i])
    print(all_precision[i])
    print(all_recall[i])
    print(all_f1[i])

    print('\n')

df = pd.DataFrame(count_list)
df['precision'] = all_precision
df['recall'] = all_recall
df.to_csv('model_stats.csv')
print(df)

vape_data = pd.read_csv('/content/vape-metadata.csv')
keep_columns = vape_data.columns[:13]
vape_data = vape_data[keep_columns]
# print(vape_data.iloc[0])

predict_text = []
for i, row in vape_data.iterrows():

    predict_text.append(preprocess_text(preprocess_text(str(row['Video_Title'])) + ' ' + preprocess_text(str(row['Description']))))
print(len(predict_text))
# predict_sequence = process_text(predict_text)
# predictions = model.predict(predict_sequence, verbose=1)
# rounded_predictions = [round(i, 2) for i in predictions]

from langdetect import DetectorFactory
DetectorFactory.seed = 0


for i, row in vape_data.iterrows():
    
    label = []
    for n in range(len(count_list)):
        if predictions[i][n] > threshold:
            label.append(str(n))
    if label == []:
        label = ['5']

    # langs = detect_langs(predict_text[i])
    # if detect(predict_text[i]) != 'en':
    #     if str(detect_langs(predict_text[i])[0])[5:8] == '999':
    #         label = ['n']
            # print(str(detect_langs(predict_text[i])[0]), predict_text[i])
    
    for position in range(len(label)):
        vape_data.at[i, 'model_1_' + str(position)] = label[position]
vape_data.to_excel('video_predictions_model_1.xlsx')

